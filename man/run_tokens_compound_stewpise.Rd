% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/misc-util-functions.R
\name{run_tokens_compound_stewpise}
\alias{run_tokens_compound_stewpise}
\title{Stepwise generate compounds from tokens}
\usage{
run_tokens_compound_stewpise(
  tokens,
  pattern,
  step = 10000,
  concatenator = "_",
  valuetype = "regex",
  ...
)
}
\arguments{
\item{tokens}{A quanteda tokens object.}

\item{pattern}{A character vector, list of character vectors, dictionary, or
collocations object.}

\item{step}{How many tokens should be processed at once? Defaults to 10000.}

\item{concatenator}{The concatenation character that will connect the words
making up the multi-word sequences. Defaults to "_".}

\item{valuetype}{The type of pattern matching: "glob" for "glob"-style
wildcard expressions; "regex" for regular expressions; or "fixed" for exact
matching. Defaults to "regex".}

\item{...}{Additonal arguments passed to the tokens_compound function.}
}
\value{
A tokens object in which the token sequences matching pattern have
been replaced by new compounded "tokens" joined by the concatenator.
}
\description{
With large amounts of text and complex (regex) patterns you
might run into problems with memory when running quanteda's tokens_compound
function. Thus this function takes a stepwise approach. The tokens are split
into certain chunks (specified by step) and then tokens_compound is run on
these smaller chunks. See also
\url{https://github.com/quanteda/quanteda/issues/1539} and the
check_pattern_performance function.
}
